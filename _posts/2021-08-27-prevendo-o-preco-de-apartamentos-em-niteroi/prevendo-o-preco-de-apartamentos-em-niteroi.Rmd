---
title: "Prevendo o Preço de Apartamentos em Niterói"
description: |
  A previsão de preços de imóveis é uma tarefa muito comum em ciência de dados, existingo até _Hello World_ para esta prática - o Ames Housing, com informações sobre o preço e outros metadados de imóveis na cidade de Ames em Iowa. Neste post, eu sigo esta ideia e utilizo um conjunto de dados reais sobre os apartamentos disponíveis para a venda no município de Niterói/RJ. Buscarei entender e prever a variação no preço destes imóveis de acordo com as informações contidas nos anúncios com a ajuda de um modelo de Machine Learning.
author:
  - first_name: Nicholas 
    last_name: Marino
    url: https://github.com/nacmarino
date: 08-27-2021
categories:
  - classicos
  - xgboost
  - tidymodels
preview: images/niteroi.jpeg
output:
  distill::distill_article:
    self_contained: false
    toc: true
    code_folding: true
    highlight: rstudio
draft: true
---

```{r setup, include=FALSE}
# setando as opções gerais dos code chunks
knitr::opts_chunk$set(echo = FALSE, code_folding = FALSE, fig.align = 'center')

# presetando o ggplot2
library(ggplot2)

# setando o tema geral do ggplot2
theme_set(new = theme_minimal(base_family = 'Roboto'))

# atualizando o tema
theme_update(
  plot.title    = element_text(face = 'bold', size = 8),
  plot.subtitle = element_text(size = 6),
  plot.caption  = element_text(size = 6),
  axis.title    = element_text(face = 'bold', size = 6),
  axis.text     = element_text(color = 'black', size = 6),
  strip.text    = element_text(face = 'bold', size = 6)
)
```

# Motivação

Há um tempo atrás eu comecei a pensar em trocar o apartamento em que moro por um maior, dado toda a questão da pandemia, _home office_ e o aumento no tamanho da família. Logo, comecei a buscar as opções disponíveis aqui em Niterói, mas me deparei com algumas dúvidas:  

- **Quanto vale o apartamento em que morava?** Dado que eu faria uma troca, eu precisava saber o quanto ele deve valer para saber até onde eu poderia buscar.  
- **Como saber se um apartamento está anunciado a um preço razoável?** Isto é, como eu poderia saber se o valor que estavam pedindo por uma apartamento está dentro do valor de mercado, acima ou abaixo dele? Ter acesso à essa informação é importante também para avaliar quão boa uma oportunidade de fato é.  

Com isso em mente, tive a ideia de pegar todas informações que podia obter de anúncios sobre o preço dos apartamentos, suas características, localização geográfica e tudo mais e utilizar algum algoritmo de Machine Learning para me ajudar (1) à prever o valor de um apartamento baseado nestas informações e (2) definir se o valor anunciado era um valor razoável ou não. Quanto a este último ponto, eu poderia até ir além: caso o valor anunciado estivesse abaixo daquele previsto pelo modelo utilizado, então possivelmente o apartamento estava sendo vendido abaixo do valor de mercado e, portanto, poderia ser um potencial bom negócio.  

Seguindo esta ideia, raspei o site do ZapImóveis atrás de todos os anúncios de apartamentos à venda no município de Niterói/RJ no início de mês de agosto. Eu organizei os dados coletados, selecionei algumas informações primárias para me ajudar nesta tarefa e, aqui neste post, eu busco entender estes dados e fazer uso deles para me ajudar a obter as respostas que estava buscando para as perguntas feitas originalmente.

# Descrição dos Dados

Vamos começar carregando os dados obtidos através do ZapImóveis, e já passados por um processo prévio de faxina (que eu não apresento aqui).

```{r carrega_dados, code_folding = TRUE}
# carregando os pacotes necessários
library(tidyverse) # manipulação de dados
library(skimr) # diagnostico rapido da base
library(reactable) # facilita a visualização de tabelas
library(sf) # mapas

# carregando os dados dos anuncios já tratados e limpos
dados <- read_rds(file = 'data/base_analitica.rds')

# carregando o shapefile de niteroi
niteroi <- read_sf('data/bairros_niteroi.shp')

# visao geral dos dados
skim_without_charts(data = dados) %>% 
  as_tibble() %>% 
  select(skim_type:complete_rate, character.n_unique, contains('numeric')) %>% 
  reactable(striped = TRUE, highlight = TRUE, compact = TRUE, showSortable = TRUE,
            style = list(fontSize = "12px"),
            defaultColDef = colDef(align = 'center'),
            columns = list(
              skim_type           = colDef(name = 'Tipo'),
              skim_variable       = colDef(name = 'Variável'),
              n_missing           = colDef(name = 'Qtd. Faltantes'),
              complete_rate       = colDef(name = 'Prop. Completa', format = colFormat(digits = 2)),
              character.n_unique  = colDef(name = 'Níveis Únicos'),
              numeric.mean        = colDef(name = 'Média', format = colFormat(digits = 0)),
              numeric.sd          = colDef(name = 'Desvio Padrão', format = colFormat(digits = 0)),
              numeric.p0          = colDef(name = 'Mínimo', format = colFormat(digits = 0)),
              numeric.p25         = colDef(name = 'Q25', format = colFormat(digits = 0)),
              numeric.p50         = colDef(name = 'Mediana', format = colFormat(digits = 0)),
              numeric.p75         = colDef(name = 'Q75', format = colFormat(digits = 0)),
              numeric.p100        = colDef(name = 'Máximo', format = colFormat(digits = 0)))
  )
```

A base carregada contém um total de `r format(x = nrow(dados), big.mark = '.', decimal.mark = ',')` anúncios de apartamentos (apenas apartamentos, não busquei casas ou outros tipos de imóveis) situados no município de Niterói/RJ. Apesar da base completa possuir cerca de 300 colunas com informações extremamente detalhadas de cada imóvel, escolhi aqui trabalhar com um subconjunto muito menor de informações, no intuito de começar simples. Ainda assim, muitas das informações presentes nesta base já têm o potencial de fornecer _insights_ importantes para a finalidade desta análise, tais como a quantidade de suítes, banheiros, quartos, vagas de garagem e área de cada apartamento. Além disso, possuímos informações de alto nível sobre a localização de cada imóvel, sendo importante notar que^[https://pt.wikipedia.org/wiki/Niter%C3%B3i]:

- Niterói é composto por 52 bairros: `r glue::glue_collapse(x = sort(unique(niteroi$Bairro)), sep = ', ', last = ' e ')`. 
- Estes bairros estão organizados entre 19 sub-regiões administrativas (_i.e._, coluna Zona na base carregada), que tendem a agregar bairros adjacentes e, de certa forma, mais similares entre si: `r glue::glue_collapse(x = sort(unique(niteroi$Zona)), sep = ', ', last = ' e ')`. 
- Por sua vez, estas sub-regiões estão organizadas entre de 5 regiões administrativas, sendo elas: `r glue::glue_collapse(x = sort(unique(niteroi$Regiao)), sep = ', ', last = ' e ')`.

Todas estas informações sobre a organização político-administrativa de Niterói podem ser melhor visualizadas através do mapa abaixo^[Shapefile obtido a partir de https://geo.niteroi.rj.gov.br/civitasgeoportal/].

```{r mapa_niteroi, code_folding = TRUE, fig.align='center'}
# carregando o leaflet
library(leaflet) # mapas interativos

# criando a paleta de cores
paleta_1 <- colorFactor(palette = 'inferno', domain = NULL, levels = sort(unique(niteroi$Regiao)))

# plotando o preco medio por Zona Administrativa
leaflet(niteroi) %>% 
  addPolygons(
    popup = paste0(
      '<b>Região Administrativa:</b> ', niteroi$Regiao, '<br>',
      '<b>Sub-Região Administrativa:</b> ', niteroi$Zona, '<br>',
      '<b>Bairro:</b> ', niteroi$Bairro, '<br>'
    ), 
    fillColor = ~ paleta_1(niteroi$Regiao),
    fillOpacity = 0.6, smoothFactor = 0.5, weight = 0.8, color = 'black') %>% 
  addTiles()
```

# Análise Exploratória

Conforme pôde ser visto na tabela de descrição da base, as únicas informações faltantes existentes estão nas colunas de latitude e longitude. Neste contexto, podemos começar olhando a distribuição espacial da nossa variável resposta, o preço do apartamento, através do bairro ao qual pertence o apartamento do anúncio. 

O mapa abaixo mostra que o preço médio dos apartamentos parece ser maior nos bairros na porção sul e sudoeste do município (Charitas, São Francisco, Icaraí e Camboinhas, por exemplo). Além disso, podemos ver que não existem apartamentos à venda em alguns bairros (polígonos brancos no mapa abaixo). Todos esses padrões são bastante consistentes com o desenvolvimento do município, onde a construção civil tem avançado bastante rápido em alguns daqueles primeiros bairros. Uma informação importante, implícita no mapa, é que o preço médio dos apartamentos parece ser muito similar entre as regiões administrativas Norte, Pendotiba e Leste, e menores do que aquele observado nas regiões Oceânica e Praias da Baia. Além disso, note que a variância nos preços entre os bairros de uma região administrativa também parece ser menor para àquelas três primeiras regiões, e maior nas duas últimas. Isto será bastante importante quando formos preparar os dados para a modelagem.

```{r mapa_preco_medio, code_folding = TRUE, fig.align='center'}
# calculando preco medio por zona
preco_por_bairro <- dados %>% 
  group_by(neighborhood) %>% 
  summarise(preco_medio = mean(price)) %>% 
  left_join(x = niteroi, y = ., by = c('Bairro' = 'neighborhood')) 

# criando a paleta de cores
paleta_2 <- colorNumeric(palette = 'viridis', domain = NULL, na.color = '#FFFFFF')

# plotando o preco medio por Bairro
leaflet(preco_por_bairro) %>% 
  addPolygons(
    popup = paste0(
      '<b>Região Administrativa:</b> ', preco_por_bairro$Regiao, '<br>',
      '<b>Sub-Região Administrativa:</b> ', preco_por_bairro$Zona, '<br>',
      '<b>Bairro:</b> ', preco_por_bairro$Bairro, '<br>',
      '<b>Preço Médio:</b> ', scales::dollar(x = preco_por_bairro$preco_medio, 
                                             prefix = 'R$ ', 
                                             big.mark = '.', 
                                             decimal.mark = ',')
    ), 
    fillColor = ~ paleta_2(preco_por_bairro$preco_medio),
    fillOpacity = 0.8, smoothFactor = 0.5, weight = 0.8, color = 'black') %>% 
  addTiles() %>% 
  addLegend(
    title = 'Preço Médio do<br>Apartamento', position = 'bottomright', pal = paleta_2, 
    values = ~ preco_por_bairro$preco_medio, bins = 4, opacity = 0.8, 
    labFormat = labelFormat(prefix = 'R$ ', big.mark = '.')
  )
```

Um segundo ponto importante a se notar é que 83.6% dos anúncios que serão analisados pertencem às sub-regiões de Icaraí, São Francisco, Santa Rosa e Centro - todas elas situadas na região administrativa 'Praias da Baia', conforme podemos observar na figura acima. Ainda assim, parece existir um forte viés em favor de anúncios no bairro de Icaraí que, sozinho, representa cerca de 50% dos anúncios na base.

```{r anuncios_por_adm, layout = 'l-page', code_folding = TRUE, fig.height=3, fig.width = 7}
# quantidade de anuncios entre regioes e sub-regioes administrativas
dados %>% 
  count(Regiao, Zona) %>% 
  mutate(proporcao = n / sum(n)) %>%
  ggplot(mapping = aes(x = reorder(Zona, -n), y = n, fill = Regiao)) +
  geom_col(color = 'black', width = 0.7, size = 0.3, show.legend = FALSE) +
  geom_text(mapping = aes(label = scales::percent(x = proporcao, accuracy = 0.01)), nudge_y = 200, size = 2) +
  scale_fill_viridis_d(option = 'B', begin = 0.1, end = 0.9) +
  labs(
    title = "Os anúncios estão concentrados entre as sub-regiões administrativas da região 'Praias da Baia'",
    x     = 'Sub-Região Administrativa',
    y     = 'Quantidade de Anúncios'
  ) +
  theme(
    axis.text.x     = element_text(angle = 90, hjust = 1, vjust = 0.5)
  )
```

De fato, a figura abaixo demonstra que para cada 55 anúncios pertencentes ao bairro de Icaraí, temos 7 anúncios no bairro de Charitas, 3 anúncios no bairro do Ingá e 1 anúncio no bairro Vital Brazil (a quantidade de casinhas para cada bairro representa a proporção relativa de anúncios entre bairros). Em resumo, (a) a região administrativa das 'Praias da Baia' está sobre-representada na base analisada, (b) alguns poucos bairros fornecem praticamente toda a informação disponível na base e (c) nem todos os bairros de Niterói estão representados dentro desta base de dados. Ests três pedaços de informação serão importantes na hora em que formos preparar o modelo preditivo.

```{r anuncios_por_bairro, layout = 'l-page', code_folding = TRUE, fig.width = 9}
library(waffle) # para o waffle plot

# representatividade dos anuncios por bairro
dados %>% 
  mutate(Bairro = fct_lump_prop(f = neighborhood, prop = 0.01, other_level = 'Outros')) %>% 
  count(Bairro) %>% 
  mutate(
    prop = n / 100
  ) %>% 
  ggplot(mapping = aes(label = Bairro, values = prop)) +
  geom_pictogram(n_rows = 15, mapping = aes(color = Bairro), 
                 make_proportional = FALSE, flip = TRUE, size = 5) +
  scale_color_viridis_d(begin = 0.1) +
  scale_label_pictogram(values = 'home') +
  coord_equal() +
  labs(
    title    = 'Razão relativa da quantidade de anúncios entre bairros',
    subtitle = '99% dos anúncios na base estão associados a estes 10 bairros, ainda que Icaraí contribua com 49.7% deste total',
    caption  = "Cada casa representa cerca de 100 anúncios associadas à cada bairro.\nA categoria 'Outros' representa o somatório dos anúncios de todos os outros 32 bairros na base analisada."
  ) +
  guides(color = guide_legend(title.vjust = 3, title.hjust = 0.3, 
                              title.theme = element_text(face = 'bold', size = 8), 
                              override.aes = list(size = 4))) +
  theme_void(base_family = 'Roboto') +
  theme(
    plot.title    = element_text(face = 'bold', size = 10),
    plot.subtitle = element_text(size = 8),
    plot.caption  = element_text(size = 8)
  )
```

A tabela de descrição dos dados logo no início deste post também apresenta mais uma curiosidade sobre quatro variáveis preditoras que estão na base...

- Parece não existir custo de condomínio para alguns apartamentos, enquanto outros informam um custo mensal que chega ao valor do próprio apartamento (painel a, abaixo). Me parece que este último caso representa algum tipo de erro de imputação de dados, enquanto o primeiro seria uma falha em reportar o valor real. Em todo caso, acredito que não dê para confiar tanto nessa informação, e vou ter que cuidar disso durante a preparação dos dados;  
- Outra coisa curiosa está no próprio tamanho dos apartamentos (painel b): existem alguns apartamento bem pequenos (kitnets, talvez?), mas o que impressiona é pensar que existem apartamentos anunciados que vão do tamanho de campos de futebol de salão (uns 500m²) à campos de tamanho oficial (próximo à 6.000m²). Como acredito ser surreal que um apartamento seja tão grande assim, e é capaz que isto também seja um erro na base;  
- Por fim, dois fatos curiosos estão nas variáveis de quantidade de banheiros (painel c) e vagas disponíveis (painel d). Existem anúncios que tratam de apartamentos com um número absurdo de banheiros (11 banheiros, sério?) e outros no qual uma única unidade parece ter disponível 8 vagas de garagem (haja carro ou espaço no prédio!). Nestes dois casos, acredito que estes valores tratam-se de erros de imputação de dados também.  

```{r dados_errados, layout = 'l-page', code_folding = TRUE}
library(patchwork) # para fazer uma composição de figuras

# distribuição da variável valor do condominio
p1 <- ggplot(data = dados, mapping = aes(x = monthlyCondoFee)) +
  geom_density(size = 0.3) +
  geom_rug(size = 0.3) +
  scale_x_continuous(trans = 'log1p', 
                     breaks = c(0, 10, 100, 3000, 50000, 600000), 
                     labels = scales::dollar_format(
                       big.mark = '.', decimal.mark = ',', prefix = 'R$ ')
  ) +
  labs(
    x        = 'Valor Mensal do Condomínio',
    y        = 'Densidade',
    subtitle = '...condomínios grátis ou ao preço de um apartamento (por mês)\nsão um tanto estranho, não?'
  )

# distribuição da variável área do apartamento
p2 <- ggplot(data = dados, mapping = aes(x = usableAreas)) +
  geom_density(size = 0.3) +
  geom_rug(size = 0.3) +
  scale_x_continuous(trans = 'log1p',
                     breaks = c(10, 40, 90, 200, 500, 2000, 6000),
                     labels = scales::label_number(
                       big.mark = '.', decimal.mark = ',', suffix = ' m²')
  ) +
  labs(
    x        = 'Área do Apartamento',
    y        = 'Densidade',
    subtitle = '...apartamentos pequenos até são comuns atualmente, mas do tamanho\nde campos de futebol fazem sentido?'
  )

# distribuição da variável quantidade de banheiros
p3 <- ggplot(data = dados, mapping = aes(x = bathrooms)) +
  geom_density(size = 0.3) +
  geom_rug(size = 0.3) +
  scale_x_continuous(breaks = seq(from = 0, to = 11, by = 1)) +
  labs(
    x        = 'Quantidade de Banheiros',
    y        = 'Densidade',
    subtitle = '...seria possível um apartamento com um número tão grande de banheiros?'
  )

# distribuição da variável quantidade de vagas
p4 <- ggplot(data = dados, mapping = aes(x = parkingSpaces)) +
  geom_density(size = 0.3) +
  geom_rug(size = 0.3) +
  scale_x_continuous(breaks = seq(from = 0, to = 8, by = 1)) +
  labs(
    x        = 'Quantidade de Vagas',
    y        = 'Densidade',
    subtitle = '...que tipo de prédio teria tantas vagas de garagem disponíveis para um\núnico apartamento?'
  )

# composição dos quatro plots
((p1 + p2) / (p3 + p4)) +
  plot_annotation(title = 'As quatro variáveis abaixo apresentam um comportamento muito incomum dadas as suas características...', 
                  tag_levels = 'a') &
  theme(
    plot.title = element_text(face = 'bold', size = 8),
    plot.tag = element_text(size = 8),
    text = element_text('Roboto')
  )
```

E, como não deixaria de faltar em toda análise exploratória, temos abaixo uma matriz com o coeficiente de correlação entre todas as variáveis preditoras numéricas. Eu decidi usar a correlação de Spearman, uma vez que acredito ser bem difícil que haja uma relação linear entre estas variáveis^[Não mostro aqui mas, de fato, a relação entre muitas delas não é linear.]. Como poderíamos esperar, muitas das correlações observadas não nos surpreendem.

```{r correlacoes, code_folding = TRUE}
library(corrr)

select(dados, where(is.numeric), -price, -latitude, -longitude) %>% 
  correlate(use = 'pairwise.complete.obs', method = 'spearman', diagonal = 1) %>% 
  stretch() %>% 
  ggplot(mapping = aes(x = x, y = y, fill = r)) +
  geom_tile() +
  geom_text(mapping = aes(label = round(x = r, digits = 2))) +
  scale_fill_gradient(low = 'white', high = 'firebrick3') +
  labs(
    fill     = 'Spearman (r)',
    title    = 'Correlação entre as variáveis preditoras numéricas'
  ) +
  theme(
    axis.title      = element_blank(),
    legend.position = 'none'
  )
```

# Modelagem

Vou começar esta parte fazendo uma copia dos dados originais e imputando `NA` nos valores estranhos daquelas quatro variáveis que falei ainda pouco.^[Dei uma lida no texto de descrição dos anúncios e parece que todos os casos que narrei aqui são erros de imputação de informação mesmo. Assim, acredito que esta solução não seja um problema.]

```{r imputa_na}
# copiando o dados
df <- dados

# removendo algumas colunas que não fazem sentido daqui para a frente - as três primeiras 
# são identificadores de linha redundantes, enquanto os dois últimos têm muitos NAs e não
# terão uso. Vou remover o texto também pois não farei uso dele nesta postagem, e a Zona
# porque o efeito dela me parece estar muito confundido com o bairro
df <- select(df, -fonte, -json_data, -row_id, -full_text, -Zona, -latitude, -longitude)

# imputando NA para casos específicos em que há evidência de erro de imputação de informações
df <- df %>% 
  mutate(
    # NA se o valor mensal do condominio for menor que R$ 50 ou maior que R$ 10K
    monthlyCondoFee = ifelse(
      test = between(x = monthlyCondoFee, left = 50, right = 10000), 
      yes = monthlyCondoFee, 
      no = NA
    ),
    # NA se a área do apartamento foi maior que 500 m²
    usableAreas     = ifelse(test = usableAreas < 500, yes = usableAreas, no = NA),
    # NA se houverem mais de 8 vagas de garagem
    parkingSpaces   = ifelse(test = parkingSpaces < 8, yes = parkingSpaces, no = NA),
    # NA se houverem mais de 10 banheiro
    bathrooms       = ifelse(test = bathrooms < 10, yes = bathrooms, no = NA)
  )
```

Com este ajuste dos dados feito^[Se você for uma pessoa curiosa, eu acabei imputando um `NA` a `r filter(df, is.na(monthlyCondoFee) | is.na(usableAreas) | is.na(parkingSpaces) | is.na(bathrooms)) %>% nrow` linhas da base], a próxima coisa a se fazer é carregar o `tidymodels` e separar os dados de treino e teste.

```{r separa_base}
library(tidymodels) # tudo relacionado à modelagem

# criando a divisão da base e estratificando através da variável resposta, a fim de garantir
# que os dados de treino e teste estarão cobrindo o mesmo intervalo de valores
set.seed(33) # reprodutibilidade
splits <- initial_split(data = df, prop = 0.8, strata = price)

# separando os dados divididos
df_train <- training(x = splits)
df_test <- testing(x = splits)
```

O treinamento de um algoritmo através do `tidymodels` pede que tenhamos os dados que serão utilizados para o treino, uma receita que descreva as etapas de pré-processamento que serão aplicadas aos dados em tempo de treino e uma instância do algoritmo. Todos estes três componentes são reunidos dentro de um _workflow_ que cuidará então do ajuste do modelo. Assim, vou seguir essa lógica, e utilizar uma validação cruzada com 5 _folds_ para treinar o algoritmo.

```{r cria_folds}
set.seed(42) # reprodutibilidade
# criando base com folds para o treino e validação do modelo
df_folds <- vfold_cv(data = df_train, v = 5, strata = price)
df_folds
```

Vou criar uma receita para o pré-processamento dos dados que vai cuidar de algumas coisas. Primeiro, vou dizer que a coluna `id` representa o identificador único de cada linha. Na sequência, vou criar um passo para que qualquer nível das variáveis categóricas `neighborhood` (_i.e._, `bairro`) e `Regiao` (_i.e._, região administrativa) que ocorram na base de teste (ou _fold_ de validação) e não ocorram na base (ou _fold_) de treino tenham um nível a parte para si próprias. Com esta etapa feita, vou tratar de agregar os níveis menos frequentes da variável `neighborhood` e, então, fazer um _one-hot-encoding_ das duas variáveis categóricas utilizadas aqui - como pretendo usar um modelo de árvore, não há problema nisso. Com isto, parto para tratar daquela imputação de `NA`s que fiz, e utilizei um KNN para imputar os valores faltantes utilizando as informações da identidade do bairro, região administrativa, quantidade de quartos e suítes. Por fim, eu aplico uma transformação para logaritimizar os valores da variável resposta.

Têm dois pontos importantes que acho legal trazer aqui. O primeiro deles está na escolha das variáveis utilizadas para a imputação: considerei aqui a premissa de que o valor médio das quatro variáveis que seriam imputadas variam em função da identidade do bairro e da região administrativa e que, além disso, mesmo dentro destes àquelas variáveis também poderiam mudar em função da quantidade de quartos e suítes. O segundo ponto é que a receita definida abaixo é apenas uma instância do pré-processamento que deve ser aplicado aos dados antes de passá-los ao algoritmo. Com isto, esta receita será aplicada aos dados (ou _folds_) de treino em tempo de ajuste do algoritmo, e não no momento que o objeto `pre_processamento` é criado - prevenindo assim que haja algum tipo de vazamento dos dados de teste para os de treino. 

```{r receita_tidymodels}
pre_processamento <- recipe(price ~ ., data = df_train) %>% 
  # dizendo para o pré-processamento que a coluna id é o identificador de cada linha
  update_role(id, new_role = 'id') %>% 
  # criando niveis novos para as variaveis bairro e regiao administrativa caso um nivel
  # que nao tenha sido usado no treinamento apareça
  step_novel(neighborhood, Regiao) %>% 
  # agrupando niveis da variavel bairro que são muito infrequentes
  step_other(neighborhood, threshold = 0.01, other = 'Outros') %>% 
  # fazendo um one hot encoding das variaveis de bairro e regiao 
  step_dummy(neighborhood, Regiao, one_hot = TRUE) %>% 
  # usando um KNN para imputar as informações faltantes nas quatro variaveis problematicas
  step_impute_knn(monthlyCondoFee, usableAreas, parkingSpaces, bathrooms, 
                  impute_with = imp_vars(contains('neighborhood'), contains('Regiao'), 
                                         'bedrooms', 'suites', -ends_with('new')), 
  ) %>% 
  # logaritimizando a variável resposta
  step_log(price)
```

O próximo passo será criar uma instância do algoritmo que será usado. No caso, vou utilizar o algoritmo `xgboost`, dada a boa performance que este tipo de algoritmo normalmente entrega nos problemas de dados. Vou utlizar o _early stopping_, a fim de reduzir a chance de que haja algum tipo de problema de sobre-ajuste do algoritmo aos dados. Finalmente, deixarei os valores de alguns hiperparâmetros do algoritmo serem definidos através de uma otimização bayesiana.

```{r instancia_xgboost}
library(xgboost) # carregando o xgboost

# criando uma instancia do xgboost
xgb_instance <- boost_tree(
  trees       = 1000, # quantidade de árvores
  tree_depth  = tune(), # profundidade das árvore
  mtry        = tune(), # quantidade de variáveis disponíveis
  sample_size = tune(), # proporção da amostra a ser usada
  learn_rate  = tune(), # taxa de aprendizado
  stop_iter   = 100
) %>% 
  set_engine(engine = 'xgboost') %>% 
  set_mode(mode = 'regression')
```

Pronto, agora temos os três principais ingredientes para criarmos um workflow de ajuste dos dados.

```{r instancia_workflow}
xgb_wf <- workflow() %>% 
  add_recipe(pre_processamento) %>% 
  add_model(xgb_instance)
```

Hora de ajustar o modelo! Para isso, vamos utilizar uma otimização bayesiana para determinar os valores dos hiperparâmetros que nos fornece o menor erro de previsão. A ideia geral da otimização bayesiana é que ela usa as informações dos valores dos hiperparâmetros e a respectiva performance obtida em cada iteração para treinar um meta-modelo que indicará qual a combinação de hiperparâmetros têm a expectativa de melhorar a sua performance e que, portanto, deverá ser utilizada na iteração seguinte. De uma forma geral, podemos pensar nesta otimização como uma busca mais inteligente pela melhor combinação de hiperparâmetros. Finalmente, um ponto importante aqui é que queremos que o modelo gerado nos entregue uma previsão com baixo erro; portanto, utilizaremos a raíz quadrada dos erros quadráticos médios (RMSE, _Root Mean Squared Error_) como a métrica de avaliação da performance do modelo. 

```{r tuna_xgboost}
# finalizando a seleção dos hipeparametros que serão testados
xgb_parameters <- parameters(xgb_instance) %>% 
  update(mtry = mtry(range = c(1L, 10L)))

# definindo a métrica que quero otimizar
metrica <- metric_set(rmse)

# buscando melhor combinação de hiperparametros através de uma busca bayesiana
set.seed(42) # reprodutibilidade
xgb_bayes <- xgb_wf %>% 
  tune_bayes(
    # dados com os folds de treino
    resamples = df_folds,
    # espaço de hiperparametros que serão otimizados
    param_info = xgb_parameters,
    # define quantas combinações de hiperparametros-performance serão geradas antes do algoritmo
    # de otimização bayesiana começar a gerar as combinações que serão de fato testadas
    initial = 10, 
    # quantidade de interações da otimização bayesiana
    iter = 5,
    # conjunto de metricas que serão avalidadas
    metrics = metrica,
    objective = exp_improve(),
    # argumentos de controle da função
    control = control_bayes(no_improve = 20, uncertain = 4, seed = 33, 
                            save_pred = TRUE, verbose = FALSE)
  )
```

Yay! Modelo ajustado! Agora, vamos avaliá-lo e tentar entendê-lo!

# Avaliação do Modelo

blablabla

```{r plota_hiperparametros, layout = 'l-body-outset'}
autoplot(object = xgb_bayes)
```

blablabla

```{r melhores_tunings}
show_best(x = xgb_bayes)
```

blablabla

``` {r modelo_naive}
# calculando o RMSE que seria obtido entre todos os folds utilizando como modelo apenas o
# valor medio do preco do apartamento
modelo_naive <- df_folds %>% 
  mutate(
    # pegando o dataframe de treino de cada fold
    train_fold       = map(.x = splits, .f = analysis),
    # pegando o dataframe de validação de cada fold
    val_fold         = map(.x = splits, .f = assessment),
    # calculando o valor do preco medio do apartamento de cada fold de treino
    preco_medio_fold = map_dbl(.x = train_fold, .f = ~ mean(.x$price)),
    # extraindo os valores do preco medio de cada fold de validação
    preco_fold_val   = map(.x = val_fold, .f = ~ .x %>% pull(price))
  ) %>% 
  # pegando so as colunas de id do fold, medio do preco no fold de treino e preco observado
  # no fold de validacao
  select(id, preco_medio_fold, preco_fold_val) %>% 
  # desaninhando a list-column do preco vindo do fold de validação
  unnest(preco_fold_val) %>% 
  # agrupamento o dataframe pelo id do fold
  group_by(id) %>% 
  # calculando o RMSE para cada fold individualmente - logaritimizando as duas colunas para
  # que os valores obtidos sejam comparáveis com aqueles dados pelo modelo dado o pre-processamento
  summarise(rmse = rmse_vec(truth = log(preco_fold_val), estimate = log(preco_medio_fold))) %>% 
  # calculando o valor do rmse medio entre os folds e o desvio padrao
  summarise(mean = mean(rmse), std_err = sd(rmse))
```

blablabla

```{r modelo_default}
# ajustando o xgboost sem fazer nenhum tipo de melhoria ao modelo
## instanciando o algoritmo com as configurações default
xgb_default <- boost_tree() %>% 
  set_engine(engine = 'xgboost') %>% 
  set_mode(mode = 'regression')

## ajustando o xgboost com configuracao default de hiperparametros aos dados dos folds
xgb_baseline <- workflow() %>%
  # adicionando o mesmo pre-processamento
  add_recipe(pre_processamento) %>% 
  # instanciando o algoritmo default
  add_model(xgb_default) %>% 
  # usando o fit resamples para realizar o ajuste em cima da validação cruzada
  fit_resamples(
    resamples = df_folds,
    metrics   = metrica, 
    control   = control_resamples(verbose = FALSE, allow_par = TRUE, save_pred = TRUE)
  )
```

blablabla

```{r compara_modelos, code_folding = TRUE}
# juntando os resultados das três estratégias em uma tibble
bind_rows('Modelo Otimizado' = select(.data = show_best(x = xgb_bayes, n = 1), mean, std_err),
          'Modelo Default'   = select(.data = collect_metrics(x = xgb_baseline), mean, std_err),
          'Abordagem Naive'  = modelo_naive, 
          .id = 'estrategia') %>% 
  # reordenando os niveis das estrategias
  mutate(estrategia = fct_relevel(.f = estrategia, 'Abordagem Naive', 'Modelo Default')) %>% 
  # plotando o valor medio e do intervalo de confiança de 95% do RMSE para cada estrategia
  ggplot(mapping = aes(x = estrategia, y = mean, fill = estrategia)) +
  geom_errorbar(mapping = aes(ymin = mean - 1.96 * std_err, ymax = mean + 1.96 * std_err), width = 0.1) +
  geom_point(size = 3, shape = 21, show.legend = FALSE) +
  geom_text(mapping = aes(label = paste0(round(x = mean, digits = 3), ' ± ', round(x = 1.96 * std_err, digits = 3))), 
            nudge_y = 0.03) +
  scale_fill_manual(values = c('white', 'dodgerblue3', 'indianred3')) +
  labs(
    title    = 'Comparação entre estratégias de previsão',
    subtitle = 'Um modelo de Machine Learning e a otimização de hiperparâmetros ajudam a resolver este problema de dados',
    x        = 'Estratégia de Previsão',
    y        = 'RMSE'
  ) +
  theme(
    plot.title    = element_text(face = 'bold', size = 12),
    plot.subtitle = element_text(size = 10),
    axis.title    = element_text(face = 'bold', size = 10),
    axis.text     = element_text(color = 'black', size = 10),
    strip.text    = element_text(face = 'bold', size = 10)
  )
```

blablabla

```{r modelo_final}
# concluindo o workflow do xgboost
wf_final <- xgb_wf %>% 
  # utilizando a melhor combinação de hiperparametros da otimização bayesiana
  finalize_workflow(parameters = select_best(x = xgb_bayes)) 

# treinando o xgboost uma ultima vez para avaliar a performance na base de teste
xgb_final <- wf_final %>% 
  # usando o last fit para ajustar o algoritmo na base de treino e escorar na base de teste
  last_fit(split = splits, metrics = metrica)

# performance na base de teste
xgb_final %>% 
  collect_metrics() 
```

blablabla

```{r avalia_residuo, layout = 'l-body-outset', fig.height=6, code_folding = TRUE}
# treinando o xgboost para termos um objeto capaz de usarmos o predict
xgb_to_predict <- fit(
  # o objeto do last_fit não pode ser usado para o predict, portanto precisamos usar a função fit com
  # este workflow finalizado a fim de que tenhamos um modelo apto a gerar predições sobre novos dados
  object = wf_final, 
  data = df_train, 
  control = control_workflow(control_parsnip = control_parsnip(verbosity = 2))
)

# pre-processando os dados de treino utilizando a receita treinada
train_baked <- bake(
  # vamos extrair a receita treinada do modelo que ajustamos e usá-la para gerar os dados pre-processados
  object = extract_recipe(x = xgb_to_predict), 
  new_data = df_train
) %>% 
  # removendo as variaveis que nao precisamos
  select(-id, -price) %>% 
  # passando para matriz de forma que o xgboost possa usa-la
  as.matrix()

# gerando previsoes na base de treino
previsoes_treino <- predict(
  # para fazer a previsão precisamos extrair o atributo fit do modelo parsnip
  object = extract_fit_parsnip(x = xgb_to_predict),
  # como new_data precisamos passar os dados já preprocessdos
  new_data = train_baked 
)

# juntando as previsões na base de teste e treino
previsoes <- xgb_final %>% 
  collect_predictions() %>% 
  # substituindo o id por um indicador de que estas são as previsões da base de teste
  mutate(id = 'Base de Teste') %>% 
  # pegando só as colunas que me serão úteis para a criação da figura
  select(id, .pred, price) %>% 
  # juntando as previsões vindas da base de treino
  bind_rows(df_train %>% 
              # pegando só a coluna do preço observado
              select(price) %>% 
              # adicionando as previsões que fiz ali em cima (a ordem das linhas é a mesma)
              mutate(previsoes_treino,
                     # logaritimizando o preco para que a comparação seja na mesma escala
                     price = log(price),
                     # criando um id para as previsões da base de treino
                     id = 'Base de Treino')) %>% 
  mutate(
    # reordenando os níveis de id
    id      = fct_relevel(.f = id, 'Base de Treino'),
    # calculando os residuos do modelo
    residuo = .pred - price
  )

# valores previstos vs obserados
p1 <- ggplot(data = previsoes, mapping = aes(x = .pred, y = price)) +
  facet_wrap(~ id) +
  geom_point(color = 'grey70', shape = 16, alpha = 0.3, size = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = 'dodgerblue3', size = 0.5) +
  labs(
    title    = 'Relação entre os valores previstos e observados para cada uma das bases',
    subtitle = 'A linha azul representa uma relação 1:1 entre os valores previstos e observados',
    x        = 'Valor Previsto',
    y        = 'Valor Observado'
  )
# histograma de distribuição dos resíduos
p2 <- ggplot(data = previsoes, mapping = aes(x = residuo)) +
  facet_wrap(~ id, scales = 'free_y') +
  geom_histogram(color = 'black', fill = 'grey80', bins = 30, stat = 'bin', size = 0.3) +
  geom_rug(size = 0.3) +
  labs(
    title    = 'Distribuição dos resíduos do modelo para cada uma das bases',
    subtitle = 'A distribuição dos resíduos parece muito próxima daquela da distribuição normal',
    x        = 'Resíduo (Previsão - Observado)',
    y        = 'Densidade'
  )

# compondo a figura
p1 / p2
```

# Entendimento do Modelo

blablabla

```{r calcula_shap}
library(fastshap) # para extrair os valores de shap do modelo

# criando função para encapsular geração da previsao
gera_previsao <- function(object, newdata) {
  predict.model_fit(object, new_data = newdata)$.pred
}

# gerando valores de shap para cada instancia
shap <- explain(
  object = extract_fit_parsnip(xgb_to_predict), 
  X = train_baked, pred_wrapper = gera_previsao
)
```

blablabla

```{r importancia_variaveis, layout = 'l-body-outset', fig.height=6, code_folding = TRUE}
autoplot(object = shap, type = 'importance') +
  labs(
    y     = 'SHAP Value (Média dos valores absolutos)',
    title = 'Importância absoluta das variáveis para o modelo'
  )
```

blablabla

```{r impacto_variaveis, layout = 'l-body-outset', fig.height=6, code_folding = TRUE}
# valores de cada uma das variaveis
valores_variaveis <- train_baked %>% 
  # passando os dados para um tibble
  as_tibble %>% 
  # adicionando um identificador sequencial para a linha
  rownames_to_column(var = 'row_id') %>% 
  # passando a tabela para o formato longo
  pivot_longer(cols = monthlyCondoFee:Regiao_new, names_to = 'variavel', values_to = 'valor') 

# criando figura do shap discriminando não só a magnitude do impacto bem como a direção do 
# impacto de cada variável sobre a previsão
shap %>% 
  # passando a tabela do shap para uma tibble
  as_tibble %>% 
  # adicionando um identificador sequencial para a linha
  rownames_to_column(var = 'row_id') %>% 
  # passando a tabela para o formato longo
  pivot_longer(cols = monthlyCondoFee:Regiao_new, names_to = 'variavel', values_to = 'shap_value') %>% 
  # juntando com os valores observados
  left_join(y = valores_variaveis, by = c('row_id', 'variavel')) %>% 
  # reordenando as variáveis através da importância do shap (media dos valores absolutos)
  mutate(
    variavel = fct_reorder(.f = variavel, .x = shap_value, .fun = function(x) mean(abs(x)))
  ) %>%
  # agrupando os valores por variavel
  group_by(variavel) %>% 
  # criando um mapa de cores através do qual rankearemos os valores observados de cada variavel
  mutate(
    # criando o rank para cada uma das variaveis
    color_map = rank(x = valor, ties.method = 'min'),
    # padronizando o rank dentro de cada variavel, caso contrário o mapa de cores vai ficar confundido
    # com quantos níveis de rank cada variável tem
    color_map = (color_map - min(color_map)) / (max(color_map) - min(color_map))
  ) %>% 
  # plotando a magnitude da contribuição e direção do impacto de cada variável
  ggplot(mapping = aes(x = shap_value, y = variavel, color = color_map)) +
  geom_vline(xintercept = 0, linetype = 2, color = 'black') +
  geom_point(position = position_jitter(height = 0.1), alpha = 0.5, size = 1) +
  scale_color_gradient(low = 'dodgerblue1', high = 'firebrick1', 
                       breaks = c(0, 1), labels = c('Min', 'Max')) +
  labs(
    x        = 'SHAP Value',
    color    = 'Valor da\nVariável',
    title    = 'Contribuição e efeito das variáveis sobre a previsão',
    subtitle = 'O valor do apartamento aumenta quanto maior for o SHAP value para uma observação, enquanto que a variação\nno valor original de uma variável de seu mínimo ao seu máximo ao longo deste gradiente indica se ela possui uma\nrelação positiva ou negativa com o valor do imóvel.'
  ) +
  theme(
    axis.title.y = element_blank(),
    legend.title = element_text(size = 8, face = 'bold', vjust = 3),
    legend.key.width = unit(x = 0.5, units = 'cm'),
    legend.key.height = unit(x = 0.5, units = 'cm')
  )
```

# Conclusões

blablabla

```{r aptos_interessantes, layout = 'l-page', code_folding = TRUE}
# pre-processando todos os dados disponíveis
all_baked <- bake(
  # vamos extrair a receita treinada do modelo que ajustamos e usá-la para gerar os dados pre-processados
  object = extract_recipe(x = xgb_to_predict), 
  new_data = df
) %>% 
  # removendo as variaveis que nao precisamos
  select(-id, -price) %>% 
  # passando para matriz de forma que o xgboost possa usa-la
  as.matrix()

# gerando previsoes para todos os dados disponiveis
previsoes_gerais <- predict(
  object = extract_fit_parsnip(xgb_to_predict), 
  new_data = all_baked
)

# identificando os 10 apartamentos que estão abaixo do valor previsto pelo modelo
df %>% 
  mutate(
    # adicionando as previsõs do modelo
    previsoes_gerais,
    # colocando as previsões na escala original
    .pred = exp(.pred),
    # calculando o resíduo na escala original
    residuo = .pred - price
  ) %>% 
  # impondo algumas restrições ao apartamento que busco: deve ser nos bairros de São Francisco 
  # ou Charitas, possuir no mínimo uma suíte e mais de 2 quartos
  filter(neighborhood %in% c('Sao Francisco', 'Charitas'), bedrooms > 2, suites > 0) %>% 
  # pegando os 10 apartamentos com o maior residuo, i.e. onde o valor predito pelo modelo é maior
  # do que o valor anunciado
  slice_max(n = 10, order_by = residuo) %>% 
  # organizando a tabela
  select(-Regiao) %>% 
  relocate(.pred, residuo, .after = price) %>% 
  # plotando a tabela
  reactable(striped = TRUE, highlight = TRUE, compact = TRUE, showSortable = TRUE,
            style = list(fontSize = "10px"),
            defaultColDef = colDef(align = 'center'),
            columns = list(
              price           = colDef(name = 'Preço anunciado', format = colFormat(currency = 'BRL', separators = TRUE, locales = 'pt-BR')),
              .pred           = colDef(name = 'Preço previsto', format = colFormat(currency = 'BRL', separators = TRUE, locales = 'pt-BR')),
              residuo         = colDef(name = 'Resíduo', format = colFormat(currency = 'BRL', separators = TRUE, locales = 'pt-BR')),
              monthlyCondoFee = colDef(name = 'Condomínio', format = colFormat(currency = 'BRL', separators = TRUE, locales = 'pt-BR')),
              tempo_anunciado = colDef(name = 'Tempo anunciado'),
              usableAreas     = colDef(name = 'Área', format = colFormat(suffix = 'm²')),
              parkingSpaces   = colDef(name = 'Vagas'),
              suites          = colDef(name = 'Suítes'),
              bathrooms       = colDef(name = 'Banheiros'),
              bedrooms        = colDef(name = 'Quartos'),
              neighborhood    = colDef(name = 'Bairro')
            )
  )
```

blablabla

```{r meu_apto_teorico}
# criando um apartamento teorico - usando as mesmas colunas que tínhamos na base original
apto_teorico <- tibble(id = 'teorico', price = NA, monthlyCondoFee = 800, 
                       tempo_anunciado = 1, usableAreas = 60, parkingSpaces = 1, 
                       suites = 1, bathrooms = 2, bedrooms = 2, neighborhood = 'Inga', 
                       Regiao = 'Praias da Baia')

apto_teorico <- tibble(id = 'teorico', price = NA, monthlyCondoFee = 976, 
                       tempo_anunciado = 1, usableAreas = 86, parkingSpaces = 2, 
                       suites = 2, bathrooms = 3, bedrooms = 2, neighborhood = 'Charitas', 
                       Regiao = 'Praias da Baia')

# extraindo a previsao do valor do meu apto teorico
predict(
  # utilizando o fit do modelo ajustado
  object = extract_fit_parsnip(xgb_to_predict), 
  # preprocessando os dados do apartamento teorico com a receita treinada
  new_data = bake(
    object = extract_recipe(x = xgb_to_predict), 
    new_data = apto_teorico) %>% 
    select(-price, -id) %>% 
    as.matrix()
) %>% 
  # extraindo apenas a previsão
  pull(.pred) %>% 
  # colocando ela de volta na escala real
  exp %>% 
  # estilizando o output
  scales::dollar(x = ., prefix = 'R$ ', big.mark = '.', decimal.mark = ',', accuracy = 0.01)
```

blablabla

# Possíveis Extensões

- blablabla  
- blablabla  
- blablabla  